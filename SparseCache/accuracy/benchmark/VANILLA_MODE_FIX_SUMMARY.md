# Vanillaæ¨¡å¼ä¹±ç é—®é¢˜ä¿®å¤æ€»ç»“

## é—®é¢˜æè¿°
åœ¨ä¸å¯ç”¨InfiniGençš„vanillaæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹è¾“å‡ºå…¨æ˜¯ä¹±ç ï¼ˆé‡å¤çš„ç¬¦å·ã€æ•°å­—ã€ç‰¹æ®Šå­—ç¬¦ï¼‰ï¼Œæ— æ³•ç”Ÿæˆæ­£å¸¸çš„æ–‡æœ¬ã€‚

ç¤ºä¾‹ä¹±ç è¾“å‡ºï¼š
```
"\n\n\n  < < <\n\n  \n\n    111\n\n                                                                                 \n\n   0011222333444555\n\n\\\\\\\n\n\t\t\t\n\n       666777888999"
```

---

## æ ¹æœ¬åŸå› åˆ†æ

### ğŸ”´ é—®é¢˜1: `apply_rotary_pos_emb`å‡½æ•°ä¸­çš„è‡´å‘½clampæ“ä½œ
**æ–‡ä»¶**: `modeling_llama_ours.py`, Lines 133-145 (ä¿®å¤å‰)

**é—®é¢˜ä»£ç **:
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    # Ensure indices are in range and on the right device/dtype
    max_len = cos.size(2)
    position_ids = position_ids.to(device=q.device, dtype=torch.long)
    position_ids = position_ids.clamp(min=0, max=max(0, max_len - 1))  # âŒ è¿™æ˜¯é—®é¢˜æ‰€åœ¨ï¼
    
    gather_indices = position_ids[:, None, :, None]
    ...
```

**ä¸ºä»€ä¹ˆå¯¼è‡´ä¹±ç **:
1. `LlamaRotaryEmbedding`åœ¨åˆå§‹åŒ–æ—¶ä¼šç¼“å­˜`max_position_embeddings`ä¸ªä½ç½®çš„cos/sinå€¼
2. å¯¹äºllama-2-7b-32kæ¨¡å‹ï¼Œ`max_position_embeddings=32768`
3. å½“æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œ`position_ids`å¯èƒ½æ˜¯[0, 1, 2, ..., 5000]
4. ä½†å¦‚æœ`cos.size(2) < 5000`ï¼ˆæ¯”å¦‚åªæœ‰2048ï¼‰ï¼Œclampä¼šæŠŠæ‰€æœ‰>2047çš„position_idséƒ½é™åˆ¶ä¸º2047
5. è¿™å¯¼è‡´position 2048, 2049, 2050...5000çš„token **å…¨éƒ¨ä½¿ç”¨ç›¸åŒçš„ä½ç½®ç¼–ç **
6. æ¨¡å‹å®Œå…¨æ— æ³•åŒºåˆ†è¿™äº›tokençš„ä½ç½®ï¼Œå¯¼è‡´è¾“å‡ºå®Œå…¨æ··ä¹±

**æ­£ç¡®çš„å®ç°**:
å®˜æ–¹ç‰ˆæœ¬**æ²¡æœ‰clampæ“ä½œ**ã€‚`LlamaRotaryEmbedding.forward()`ä¼šåœ¨éœ€è¦æ—¶è‡ªåŠ¨æ‰©å±•cos/sinç¼“å­˜ï¼ˆLines 112-119ï¼‰ï¼š
```python
if seq_len > self.max_seq_len_cached:
    self.max_seq_len_cached = seq_len
    t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
    freqs = torch.einsum("i,j->ij", t, self.inv_freq)
    ...
```

### ğŸŸ¡ é—®é¢˜2: Attention maskå¤„ç†é€»è¾‘é”™è¯¯
**æ–‡ä»¶**: `modeling_llama_ours.py`, Lines 346-359 (ä¿®å¤å‰)

**é—®é¢˜ä»£ç **:
```python
if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
    # Create proper causal mask when size mismatch occurs
    causal_mask = torch.triu(
        torch.full((q_len, kv_seq_len), float("-inf"), ...),
        diagonal=1  # è¿™ä¸ªå€¼åœ¨æŸäº›æƒ…å†µä¸‹æ˜¯é”™çš„
    )
    attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)
    attn_weights = attn_weights + attention_mask
```

**é—®é¢˜**:
1. è¯•å›¾åœ¨`LlamaAttention`å±‚æ‰‹åŠ¨åˆ›å»ºcausal mask
2. `attention_mask`åº”è¯¥å·²ç»åœ¨`LlamaModel.forward()`ä¸­é€šè¿‡`_prepare_decoder_attention_mask()`æ­£ç¡®å‡†å¤‡
3. å¦‚æœsizeä¸åŒ¹é…ï¼Œè¯´æ˜æœ‰bugï¼Œåº”è¯¥æŠ¥é”™è€Œä¸æ˜¯å°è¯•ä¿®å¤

**æ­£ç¡®çš„å®ç°**:
åƒå®˜æ–¹ç‰ˆæœ¬ä¸€æ ·ï¼Œå¦‚æœsizeä¸åŒ¹é…å°±ç›´æ¥æŠ›å‡ºé”™è¯¯ï¼š
```python
if attention_mask is not None:
    if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
        raise ValueError(...)
    attn_weights = attn_weights + attention_mask
```

---

## ä¿®å¤å†…å®¹

### âœ… ä¿®å¤1: åˆ é™¤apply_rotary_pos_embä¸­çš„clampæ“ä½œ
**æ–‡ä»¶**: `modeling_llama_ours.py`, Lines 133-140

**ä¿®æ”¹å‰** (Lines 133-145):
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    # Ensure indices are in range and on the right device/dtype
    max_len = cos.size(2)
    position_ids = position_ids.to(device=q.device, dtype=torch.long)
    position_ids = position_ids.clamp(min=0, max=max(0, max_len - 1))
    
    gather_indices = position_ids[:, None, :, None]
    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])
    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)
    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

**ä¿®æ”¹å** (Lines 133-140):
```python
def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]
    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])
    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)
    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

**å…³é”®æ”¹è¿›**:
- âœ… åˆ é™¤äº†`max_len = cos.size(2)`
- âœ… åˆ é™¤äº†`position_ids.to(device=q.device, dtype=torch.long)`ï¼ˆä¸éœ€è¦ï¼Œposition_idså·²ç»åœ¨æ­£ç¡®çš„deviceä¸Šï¼‰
- âœ… **åˆ é™¤äº†è‡´å‘½çš„`position_ids.clamp(...)`æ“ä½œ**
- âœ… ç°åœ¨ä¸å®˜æ–¹transformerså®ç°å®Œå…¨ä¸€è‡´

### âœ… ä¿®å¤2: ä¿®æ­£attention maskå¤„ç†é€»è¾‘
**æ–‡ä»¶**: `modeling_llama_ours.py`, Lines 340-352

**ä¿®æ”¹å‰** (Lines 346-359):
```python
# Only apply standard attention_mask if infinigen mask was not applied
if attn_mask is None:
    if attention_mask is not None:
        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
            # Create proper causal mask when size mismatch occurs
            causal_mask = torch.triu(
                torch.full((q_len, kv_seq_len), float("-inf"), ...),
                diagonal=1
            )
            attention_mask = causal_mask.unsqueeze(0).unsqueeze(0)
        
        attn_weights = attn_weights + attention_mask
        attn_weights = torch.max(...)
```

**ä¿®æ”¹å** (Lines 340-352):
```python
# Apply mask: use infinigen mask if available, otherwise use standard attention_mask
if attn_mask is not None:
    # InfiniGen mode: use the computed sparse mask
    attn_weights = attn_weights + attn_mask
    attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))
elif attention_mask is not None:
    # Vanilla mode: use standard attention mask
    if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
        raise ValueError(
            f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
        )
    attn_weights = attn_weights + attention_mask
    attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))
```

**å…³é”®æ”¹è¿›**:
- âœ… åˆ é™¤äº†æ‰‹åŠ¨åˆ›å»ºcausal maskçš„é€»è¾‘
- âœ… å¦‚æœsizeä¸åŒ¹é…ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯ï¼ˆä¸å®˜æ–¹å®ç°ä¸€è‡´ï¼‰
- âœ… ä½¿ç”¨`if-elif`ç»“æ„ï¼Œç¡®ä¿infinigen maskå’Œæ ‡å‡†maskä¸ä¼šåŒæ—¶åº”ç”¨
- âœ… æ¸…æ™°çš„æ³¨é‡Šè¯´æ˜ä¸¤ç§æ¨¡å¼

### âœ… ä¿®å¤3: ä¿®æ­£skewing_matrixå˜é‡å
**æ–‡ä»¶**: `modeling_llama_ours.py`, Line 196

**ä¿®æ”¹å‰**:
```python
self.skewing_matrx = None  # æ‹¼å†™é”™è¯¯
```

**ä¿®æ”¹å**:
```python
self.skewing_matrix = None  # Fixed typo: was skewing_matrx
```

---

## æµ‹è¯•éªŒè¯

### æµ‹è¯•å‘½ä»¤
```bash
cd /root/sparse-load/SparseCache/accuracy/benchmark

# æµ‹è¯•vanillaæ¨¡å¼ï¼ˆä¸å¯ç”¨InfiniGenï¼‰
python longbench_pred.py \
    --model llama-2-7b-inst-32k \
    --model_type llama \
    --datasets qasper \
    --name vanilla-fixed-test
```

### é¢„æœŸç»“æœ
- âœ… æ¨¡å‹åº”è¯¥è¾“å‡ºæ­£å¸¸çš„è‹±æ–‡æ–‡æœ¬
- âœ… ä¸ä¼šå‡ºç°é‡å¤çš„ç¬¦å·ã€æ•°å­—
- âœ… è¾“å‡ºä¸é—®é¢˜ç›¸å…³ä¸”æœ‰æ„ä¹‰

### å¦‚ä½•ç¡®è®¤ä¿®å¤æˆåŠŸ
æ£€æŸ¥ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶ï¼š
```bash
cat pred/llama-2-7b-inst-32k/vanilla-fixed-test/qasper.jsonl | head -1
```

åº”è¯¥çœ‹åˆ°æ­£å¸¸çš„æ–‡æœ¬è¾“å‡ºï¼Œç±»ä¼¼ï¼š
```json
{"length": 3141, "pred": "Based on the paper, the authors...", "answers": [...]}
```

è€Œä¸æ˜¯ä¹±ç ï¼š
```json
{"length": 3141, "pred": "\n\n\n  < < <\n\n    111\n\n...", "answers": [...]}
```

---

## æŠ€æœ¯æ·±å…¥è§£é‡Š

### ä¸ºä»€ä¹ˆclampä¼šå¯¼è‡´å¦‚æ­¤ä¸¥é‡çš„é—®é¢˜ï¼Ÿ

1. **Rotary Position Embeddingçš„å·¥ä½œåŸç†**:
   - RoPEå°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°queryå’Œkeyå‘é‡ä¸­
   - æ¯ä¸ªä½ç½®éƒ½æœ‰å”¯ä¸€çš„(cos, sin)å€¼
   - åœ¨attentionè®¡ç®—æ—¶ï¼Œç›¸åŒä½ç½®çš„tokenä¼šæœ‰æ›´é«˜çš„attention score

2. **Clampçš„ç ´åæ€§å½±å“**:
   ```python
   # å‡è®¾max_len=2048ï¼Œä½†å®é™…åºåˆ—é•¿åº¦=5000
   position_ids = [0, 1, 2, ..., 2047, 2048, 2049, ..., 4999]
   
   # Clampåï¼š
   position_ids = [0, 1, 2, ..., 2047, 2047, 2047, ..., 2047]
   #                                     ^^^^^^^^^^^^^^^^^^^^
   #                                     æ‰€æœ‰è¿™äº›ä½ç½®éƒ½è¢«è®¾ä¸º2047ï¼
   ```

3. **å¯¹æ¨¡å‹çš„å½±å“**:
   - ä½ç½®2048-4999çš„æ‰€æœ‰tokenéƒ½æœ‰ç›¸åŒçš„ä½ç½®ç¼–ç 
   - æ¨¡å‹æ— æ³•åŒºåˆ†è¿™äº›tokençš„é¡ºåº
   - Attentionæœºåˆ¶å®Œå…¨æ··ä¹±
   - æ¨¡å‹å¼€å§‹ç”Ÿæˆéšæœº/é‡å¤çš„token

### ä¸ºä»€ä¹ˆå®˜æ–¹å®ç°ä¸éœ€è¦clampï¼Ÿ

å®˜æ–¹`LlamaRotaryEmbedding`ç±»æœ‰åŠ¨æ€æ‰©å±•æœºåˆ¶ï¼ˆLines 112-119ï¼‰ï¼š
```python
def forward(self, x, seq_len=None):
    if seq_len > self.max_seq_len_cached:
        self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=x.device, ...)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], ...)
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], ...)
```

å½“é‡åˆ°æ›´é•¿çš„åºåˆ—æ—¶ï¼Œä¼šè‡ªåŠ¨é‡æ–°è®¡ç®—å¹¶ç¼“å­˜æ›´å¤šä½ç½®çš„cos/sinå€¼ã€‚

---

## æ€»ç»“

### ä¿®å¤å‰çš„é—®é¢˜
1. âŒ `apply_rotary_pos_emb`ä¸­çš„clampå¯¼è‡´é•¿åºåˆ—ä½ç½®ç¼–ç é”™è¯¯ â†’ **ä¹±ç è¾“å‡º**
2. âŒ Attention maskå¤„ç†é€»è¾‘å°è¯•æ‰‹åŠ¨åˆ›å»ºmaskè€Œä¸æ˜¯trustä¸Šå±‚å‡†å¤‡çš„mask
3. âŒ å˜é‡åæ‹¼å†™é”™è¯¯ï¼ˆskewing_matrxï¼‰

### ä¿®å¤åçš„çŠ¶æ€
1. âœ… åˆ é™¤äº†è‡´å‘½çš„clampæ“ä½œï¼Œä½¿ç”¨æ ‡å‡†çš„RoPEå®ç°
2. âœ… ä¿®æ­£äº†attention maskå¤„ç†é€»è¾‘ï¼Œä¸å®˜æ–¹å®ç°ä¸€è‡´
3. âœ… ä¿®æ­£äº†å˜é‡åæ‹¼å†™
4. âœ… InfiniGené€»è¾‘ä¿æŒæ³¨é‡ŠçŠ¶æ€ï¼Œç¡®ä¿vanillaæ¨¡å¼ç¨³å®šå·¥ä½œ

### åç»­å·¥ä½œ
åœ¨vanillaæ¨¡å¼éªŒè¯æ­£å¸¸åï¼Œå¯ä»¥ï¼š
1. å–æ¶ˆæ³¨é‡ŠInfiniGené€»è¾‘
2. æµ‹è¯•InfiniGenæ¨¡å¼
3. å¯¹æ¯”vanilla vs infinigençš„æ€§èƒ½å’Œå‡†ç¡®ç‡

---

## æ–‡ä»¶æ¸…å•
- `modeling_llama_ours.py` - ä¸»è¦ä¿®æ”¹æ–‡ä»¶
- `VANILLA_MODE_FIX_SUMMARY.md` - æœ¬æ–‡æ¡£
- `INFINIGEN_FIX_SUMMARY.md` - InfiniGenå¯ç”¨æŒ‡å—ï¼ˆå¾…vanillaæ¨¡å¼éªŒè¯åä½¿ç”¨ï¼‰
