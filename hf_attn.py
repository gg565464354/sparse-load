import torch
import torch.nn.functional as F
import time
from collections import defaultdict
import json
from transformers.modeling_flash_attention_utils import _flash_attention_forward


# **ğŸš€ 1. æ ‡å‡† SDPAï¼ˆå¸¦ Maskï¼‰**
def sdpa_with_mask(Q, K_padded, V_padded, attn_mask):
    # return F.scaled_dot_product_attention(Q, K_padded, V_padded, attn_mask=attn_mask)

    _flash_attention_forward(Q, K_padded, V_padded, attention_mask=attn_mask, query_length=1, is_causal=True)

# **ğŸš€ 2. ç›´æ¥å»æ‰ Mask çš„ SDPA**
def sdpa_no_mask(Q, K_padded, V_padded):
    return F.scaled_dot_product_attention(Q, K_padded, V_padded)  # ğŸš€ æœ€é«˜æ•ˆï¼ˆå¦‚æœ mask ä»…ç”¨äº paddingï¼‰

# **ğŸš€ 3. ä»…è£å‰ª K/Vï¼Œè€Œä¸ä½¿ç”¨ Mask**
def sdpa_trimmed_kv(Q, K_padded, V_padded):
    outputs = []
    for head_idx, key_len in enumerate(key_lengths):
        Q_head = Q[:, head_idx:head_idx+1, :, :]  # å–è¯¥ head çš„ Q
        K_head = K_padded[:, head_idx:head_idx+1, :key_len, :]  # åªå–æœ‰æ•ˆçš„ K
        V_head = V_padded[:, head_idx:head_idx+1, :key_len, :]  # åªå–æœ‰æ•ˆçš„ V

        output = F.scaled_dot_product_attention(Q_head, K_head, V_head)  # è®¡ç®— SDPA
        outputs.append(output)

    return torch.cat(outputs, dim=1)  # é‡æ–°æ‹¼æ¥æ‰€æœ‰ head è®¡ç®—ç»“æœ


# **ğŸš€ 4. Grouped SDPAï¼ˆé’ˆå¯¹ä¸åŒ Key é•¿åº¦åˆ†ç»„è®¡ç®—ï¼‰**
def sdpa_grouped(Q, K_padded, V_padded):
    grouped_heads = defaultdict(list)
    for head_idx, length in enumerate(key_lengths):
        grouped_heads[length].append(head_idx)

    outputs = []
    for length, head_list in grouped_heads.items():
        Q_group = Q[:, head_list, :, :]
        K_group = K_padded[:, head_list, :length, :]
        V_group = V_padded[:, head_list, :length, :]

        output = F.scaled_dot_product_attention(Q_group, K_group, V_group)
        outputs.append(output)

    return torch.cat(outputs, dim=1)

# **ğŸš€ æµ‹è¯•æ€§èƒ½**
def benchmark(func, *args):
    # é¢„çƒ­
    func(*args)
    torch.cuda.synchronize()

    # æ­£å¼æµ‹è¯•
    start_time = time.time()
    for _ in range(10):  # è¿è¡Œ 10 æ¬¡å–å¹³å‡å€¼
        func(*args)
    torch.cuda.synchronize()
    end_time = time.time()

    cost = end_time - start_time

    print(f"{func.__name__} è¿è¡Œæ—¶é—´: {(cost) * 1000:.2f} ms")
    return cost


# é…ç½®å‚æ•°
BATCH_SIZE = 1
NUM_HEADS = 16
QUERY_LEN = 1
DIM = 128

for i in range(1, 11):
    max_key_len = int(1024*i)
    min_key_len = int(512*i)

    # æ¨¡æ‹Ÿä¸åŒ head å…·æœ‰ä¸åŒ key-value é•¿åº¦çš„æƒ…å†µ
    key_lengths = torch.randint(min_key_len, max_key_len, (NUM_HEADS,)).tolist()  # æ¯ä¸ª head çš„ key é•¿åº¦


    # ç”Ÿæˆ Query, Key, Value
    Q = torch.randn(BATCH_SIZE, NUM_HEADS, QUERY_LEN, DIM, device="cuda", dtype=torch.float16)
    K_padded = torch.zeros(BATCH_SIZE, NUM_HEADS, max_key_len, DIM, device="cuda", dtype=torch.float16)
    V_padded = torch.zeros(BATCH_SIZE, NUM_HEADS, max_key_len, DIM, device="cuda", dtype=torch.float16)

    # ç”Ÿæˆ Mask
    attn_mask = torch.zeros(BATCH_SIZE, NUM_HEADS, QUERY_LEN, max_key_len, device="cuda", dtype=torch.float16)

    # é€ä¸ªå¡«å……ä¸åŒ head çš„ key-valueï¼Œå¹¶ç”Ÿæˆ mask
    for i, length in enumerate(key_lengths):
        K_padded[:, i, :length, :] = torch.randn(BATCH_SIZE, 1, length, DIM, device="cuda", dtype=torch.float16)
        V_padded[:, i, :length, :] = torch.randn(BATCH_SIZE, 1, length, DIM, device="cuda", dtype=torch.float16)
        attn_mask[:, i, :, length:] = float("-inf")  # Mask padding éƒ¨åˆ†


    # **ğŸš€ è¿è¡Œæ‰€æœ‰æµ‹è¯•**
    print("\n=== SDPA æ€§èƒ½æµ‹è¯• ===")
    hete_attn = benchmark(sdpa_with_mask, Q, K_padded, V_padded, attn_mask)
    common_attn = benchmark(sdpa_no_mask, Q, K_padded, V_padded)
    # benchmark(sdpa_trimmed_kv, Q, K_padded, V_padded)
    # benchmark(sdpa_grouped, Q, K_padded, V_padded)

    result = {"hete_attn": hete_attn, "common_attn":common_attn}

    with open("../cache_test/result/cache_attn_cost.jsonl", "a", encoding="utf-8") as file:
        file.write(json.dumps(result, ensure_ascii=False) + "\n")
