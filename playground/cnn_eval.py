# evaluate_sparsity.py
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import evaluate
import time
import json
import os
from tqdm import tqdm

# --- é…ç½® ---
# æ¨¡å‹ID
MODEL_ID = "/share/models/opt-6.7b"
# æ•°æ®é›†ID
DATASET_ID = "cnn_dailymail"
DATASET_CONFIG = "3.0.0"
# ç”¨äºè¯„æµ‹çš„æ ·æœ¬æ•°é‡ï¼ˆè®¾ç½®ä¸º None å°†è¯„æµ‹æ•´ä¸ªæµ‹è¯•é›†ï¼‰
NUM_SAMPLES = 10
# ç”Ÿæˆæ‘˜è¦çš„æœ€å¤§ token æ•°
MAX_NEW_TOKENS = 150
# è¾“å‡ºç»“æœæ–‡ä»¶å
OUTPUT_FILENAME = "evaluation_results.json"


def check_local_model_file():
    """æ£€æŸ¥å½“å‰ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨ä¿®æ”¹è¿‡çš„æ¨¡å‹æ–‡ä»¶"""
    if not os.path.exists("modeling_opt.py"):
        raise FileNotFoundError(
            "é”™è¯¯ï¼šæœªåœ¨å½“å‰ç›®å½•ä¸‹æ‰¾åˆ° 'modeling_opt.py' æ–‡ä»¶ã€‚\n"
            "è¯·å°†æ‚¨ä¿®æ”¹åçš„æ¨¡å‹ä»£ç ä¿å­˜ä¸º 'modeling_opt.py' å¹¶ä¸æœ¬è„šæœ¬æ”¾åœ¨åŒä¸€ç›®å½•ã€‚"
        )
    print("âœ… æˆåŠŸæ‰¾åˆ° 'modeling_opt.py'ï¼Œå°†ä½¿ç”¨æœ¬åœ°ä¿®æ”¹ç‰ˆçš„æ¨¡å‹ä»£ç ã€‚")


def create_prompt(sample):
    """ä¸º CNN/DailyMail æ•°æ®é›†æ„å»ºæ ‡å‡†çš„ zero-shot prompt"""
    return f"Article: {sample['article']}\n\nSummarize the above article in a few sentences.\n\nSummary:"


def evaluate_model():
    """
    åŠ è½½æ¨¡å‹å’Œæ•°æ®é›†ï¼Œæ‰§è¡Œç”Ÿæˆä»»åŠ¡ï¼Œå¹¶è¯„æµ‹æ€§èƒ½ã€ç¼“å­˜æ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚
    """
    # 1. åŠ è½½ Tokenizer å’Œæ¨¡å‹
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_ID}...")
    # Transformers ä¼šä¼˜å…ˆåŠ è½½åŒç›®å½•ä¸‹çš„ modeling_*.py æ–‡ä»¶ï¼Œä»è€Œä½¿ä½ çš„ä¿®æ”¹ç”Ÿæ•ˆ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto", # è‡ªåŠ¨å°†æ¨¡å‹åˆ†å‘åˆ°å¯ç”¨è®¾å¤‡ (GPU/CPU)
    )
    # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # ä½ çš„ä»£ç åœ¨ OPTAttention ä¸­æ·»åŠ äº† heavy_hitter_masker
    # æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å®ƒæ˜¯å¦å·²æˆåŠŸåŠ è½½
    # try:
    #     # è®¿é—®æ¨¡å‹æ·±å±‚ç»“æ„æ¥ç¡®è®¤
    #     _ = model.model.decoder.layers[3].self_attn.heavy_hitter_masker
    #     print("âœ… è‡ªå®šä¹‰æ¨¡å— 'heavy_hitter_masker' å·²æˆåŠŸåŠ è½½ã€‚")
    # except AttributeError:
    #     print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°è‡ªå®šä¹‰æ¨¡å— 'heavy_hitter_masker'ã€‚è¯·ç¡®ä¿ 'modeling_opt.py' çš„ä¿®æ”¹å·²ç”Ÿæ•ˆã€‚")


    # 2. åŠ è½½å’Œå‡†å¤‡æ•°æ®é›†
    print(f"ğŸ“š æ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_ID}...")
    dataset = load_dataset(DATASET_ID, DATASET_CONFIG, split='test')
    
    if NUM_SAMPLES is not None:
        dataset = dataset.select(range(NUM_SAMPLES))
        print(f"é€‰æ‹©äº† {NUM_SAMPLES} ä¸ªæ ·æœ¬è¿›è¡Œè¯„æµ‹ã€‚")


    # 3. æ‰§è¡Œç”Ÿæˆå’Œè¯„æµ‹
    predictions = []
    references = []
    generation_times = []

    print("\nğŸ” å¼€å§‹ç”Ÿæˆæ‘˜è¦å¹¶æ”¶é›†ç»Ÿè®¡æ•°æ®...")
    
    # åœ¨è¯„æµ‹å¼€å§‹å‰ï¼Œé‡ç½®ä½ æ·»åŠ çš„ç»Ÿè®¡æ•°æ®
    # æ ¹æ®ä½ çš„ä»£ç ç»“æ„ï¼Œè¯¥æ–¹æ³•åœ¨ OPTModel -> OPTDecoder ä¸­
    if hasattr(model, 'model') and hasattr(model.model, 'reset_cache_hit_stats'):
        model.model.reset_cache_hit_stats()
        print("ğŸ”„ï¸ ç¼“å­˜å‘½ä¸­ç‡ç»Ÿè®¡å·²é‡ç½®ã€‚")
    else:
        print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° `reset_cache_hit_stats` æ–¹æ³•ã€‚æ— æ³•é‡ç½®ç»Ÿè®¡ä¿¡æ¯ã€‚")
    
    for sample in tqdm(dataset, desc="Generating Summaries"):
        prompt = create_prompt(sample)
        inputs = tokenizer(prompt, return_tensors="pt", max_length=1024, truncation=True).to(model.device)
        
        start_time = time.time()
        # ä½¿ç”¨ model.generate() æ¥è§¦å‘è§£ç å¾ªç¯
        with torch.no_grad():
            generated_ids = model.generate(
                inputs.input_ids,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False, # ä½¿ç”¨è´ªå¿ƒè§£ç ä»¥è·å¾—ç¡®å®šæ€§ç»“æœ
                pad_token_id=tokenizer.eos_token_id
            )
        end_time = time.time()
        
        # è®°å½•ç”Ÿæˆæ—¶é—´å’Œç»“æœ
        generation_times.append(end_time - start_time)
        
        # è§£ç ç”Ÿæˆçš„ tokenï¼Œè·³è¿‡ prompt éƒ¨åˆ†
        output_ids = generated_ids[0, inputs.input_ids.shape[1]:]
        prediction = tokenizer.decode(output_ids, skip_special_tokens=True).strip()
        
        predictions.append(prediction)
        references.append(sample['highlights'])
        
    # 4. æ”¶é›†å’Œåˆ†æç»“æœ
    print("\nğŸ“Š è¯„æµ‹å®Œæˆï¼Œæ­£åœ¨è®¡ç®—æœ€ç»ˆç»“æœ...")
    
    # 4.1 è·å–ç¼“å­˜å’Œç¨€ç–åŒ–ç»Ÿè®¡
    # hit_report = {}
    # if hasattr(model, 'model') and hasattr(model.model, 'get_cache_hit_report'):
    #     hit_report = model.model.get_cache_hit_report()
    #     print("\n--- ç¨€ç–æ³¨æ„åŠ›ä¸ç¼“å­˜ç»Ÿè®¡ ---")
    #     print(json.dumps(hit_report, indent=2))
    # else:
    #     print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° `get_cache_hit_report` æ–¹æ³•ã€‚")


    # 4.2 è®¡ç®— ROUGE åˆ†æ•°
    rouge = evaluate.load('rouge')
    rouge_results = rouge.compute(predictions=predictions, references=references)
    print("\n--- æ¨¡å‹æ€§èƒ½ (ROUGE) ---")
    for key, value in rouge_results.items():
        print(f"{key}: {value:.4f}")

    # 4.3 è®¡ç®—å¹³å‡æ¨ç†å»¶è¿Ÿ
    avg_latency = sum(generation_times) / len(generation_times)
    total_time = sum(generation_times)
    print("\n--- æ¨ç†é€Ÿåº¦ ---")
    print(f"å¤„ç†æ ·æœ¬æ€»æ•°: {len(generation_times)}")
    print(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"å¹³å‡æ¯ä¸ªæ ·æœ¬ç”Ÿæˆè€—æ—¶: {avg_latency:.3f} ç§’")

    # 5. ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
    final_results = {
        "model_id": MODEL_ID,
        "num_samples": NUM_SAMPLES or len(dataset),
        "sparsity_cache_stats": hit_report,
        "rouge_scores": rouge_results,
        "performance": {
            "average_latency_sec": avg_latency,
            "total_time_sec": total_time,
        }
    }
    
    with open(OUTPUT_FILENAME, "w") as f:
        json.dump(final_results, f, indent=4)
    print(f"\nâœ… æ‰€æœ‰è¯„æµ‹ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILENAME}")


if __name__ == "__main__":
    # check_local_model_file()
    evaluate_model()