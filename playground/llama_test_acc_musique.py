import sys
import os
import torch
import time
import json
import numpy as np
local_lib_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "libs")
sys.path.insert(0, local_lib_path)
from transformers import AutoTokenizer, AutoModelForCausalLM
import collections
import string
import re

# æ·»åŠ  libs è·¯å¾„
local_lib_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "libs")
sys.path.insert(0, local_lib_path)

def load_dataset(dataset_path):
    """åŠ è½½æµ‹è¯•æ•°æ®é›†"""
    print("Loading dataset:", dataset_path)
    with open(dataset_path) as f:
        return json.load(f)

def normalize_answer(s):
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ ¼å¼"""
    def remove_articles(text):
        return re.sub(r"\b(a|an|the)\b", " ", text)

    def white_space_fix(text):
        return " ".join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return "".join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))

def parse_generation(s):
    """è§£æç”Ÿæˆçš„æ–‡æœ¬"""
    s = s.lstrip('\n').split('\n')[0]
    
    # æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºç©ºæˆ–æ²¡æœ‰å•è¯
    if not s or not s.split():
        return s
    
    if s.startswith("Yes") or s.startswith("yes"):
        s = "Yes"
    elif s.split()[0].startswith("No") or s.split()[0].startswith("no"):
        s = "No"
    return s

def compute_f1(a_pred, a_gold, tokenizer):
    """è®¡ç®— F1 åˆ†æ•°"""
    a_pred = parse_generation(a_pred)
    gold_toks = tokenizer.encode(normalize_answer(a_gold))[1:]
    pred_toks = tokenizer.encode(normalize_answer(a_pred))[1:]
    
    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)
    num_same = sum(common.values())
    
    if len(gold_toks) == 0 or len(pred_toks) == 0:
        return int(gold_toks == pred_toks)
    if num_same == 0:
        return 0
    
    precision = 1.0 * num_same / len(pred_toks)
    recall = 1.0 * num_same / len(gold_toks)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def build_qa_prompt(example, system_prompt="è¯·æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç®€æ´å›ç­”é—®é¢˜ï¼Œç­”æ¡ˆæ§åˆ¶åœ¨5ä¸ªè¯ä»¥å†…ã€‚"):
    """æ„å»ºé—®ç­”æç¤º"""
    q = example["question"]
    docs_text = "\n\n".join([f"{ctx['title']}\n{ctx['text']}" for ctx in example["ctxs"]])
    
    # ç›´æ¥æ„å»ºpromptå­—ç¬¦ä¸²ï¼Œä¸ä½¿ç”¨chat template
    prompt = f"{system_prompt}\n\nä¸Šä¸‹æ–‡ï¼š\n{docs_text}\n\né—®é¢˜ï¼š{q}\n\nç­”æ¡ˆï¼š"
    
    return prompt

def test_model_accuracy(model_path, dataset_path):
    """æµ‹è¯•æ¨¡å‹ç²¾åº¦"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨fp16èŠ‚çœæ˜¾å­˜ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        device_map="auto",
        torch_dtype=torch.float16,  # ğŸš¨ å…³é”®ï¼šä½¿ç”¨fp16
        attn_implementation="eager",
        low_cpu_mem_usage=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # ===== ä¼˜åŒ–çš„InfiniGené…ç½® =====
    print("å¯ç”¨InfiniGenä¼˜åŒ–...")
    
    skewing_matrix_path = "/workspace/SparseCache/accuracy/setup/skewing_matrix/Llama-2-7b-hf.pt"
    partial_weight_path = "/workspace/SparseCache/accuracy/setup/weights/Llama-2-7b-hf_0.2"
    
    # ğŸš¨ ä¸è¦ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰skewing matrix
    model_dtype = torch.float16  # å¼ºåˆ¶ä½¿ç”¨fp16
    
    print("å¼€å§‹é€å±‚é…ç½®InfiniGenå‚æ•°...")
    for layer in range(len(model.model.layers)):
        if layer % 8 == 0:
            print(f"é…ç½®ç¬¬ {layer} å±‚...")
            
        model.model.layers[layer].self_attn.partial_weight_ratio = 0.2
        
        # åŠ è½½partial_weight_q
        try:
            partial_weight_q = torch.load(
                f"{partial_weight_path}/partial_weight_q_{layer}.pt",
                map_location='cpu'
            ).to(device='cuda', dtype=model_dtype)
            model.model.layers[layer].self_attn.partial_weight_q = partial_weight_q
        except Exception as e:
            print(f"è­¦å‘Šï¼šå±‚ {layer} partial_weight_q åŠ è½½å¤±è´¥: {e}")
            continue
        
        # å•ç‹¬åŠ è½½å½“å‰å±‚çš„skewing matrix
        try:
            A_full = torch.load(skewing_matrix_path, map_location='cpu')
            skewing_matrix = A_full[layer].to(device='cuda', dtype=model_dtype)
            model.model.layers[layer].self_attn.skewing_matrix = skewing_matrix
            del A_full  # ç«‹å³é‡Šæ”¾
        except Exception as e:
            print(f"è­¦å‘Šï¼šå±‚ {layer} skewing_matrix åŠ è½½å¤±è´¥: {e}")
            continue
        
        model.model.layers[layer].self_attn.alpha = 5
        model.model.layers[layer].self_attn.capacity = 1.0
        model.model.layers[layer].self_attn.budget = 0.2
        
        # æ¯é…ç½®8å±‚æ¸…ç†ä¸€æ¬¡ç¼“å­˜
        if (layer + 1) % 8 == 0:
            torch.cuda.empty_cache()
    
    print("InfiniGené…ç½®å®Œæˆ")
    torch.cuda.empty_cache()
    
    # åŠ è½½æ•°æ®é›†
    eval_dataset = load_dataset(dataset_path)
    
    f1_scores = []
    generation_times = []
    
    print(f"å¼€å§‹æµ‹è¯•ï¼Œæ ·æœ¬æ•°é‡: {len(eval_dataset)}")
    
    for i, example in enumerate(eval_dataset):
        if i % 10 == 0:
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªæ ·æœ¬...")
        
        # æ„å»ºæç¤º
        prompt = build_qa_prompt(example)
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # ç”Ÿæˆç­”æ¡ˆå¹¶è®¡æ—¶
        if device == "cuda":
            torch.cuda.synchronize()
        start_time = time.perf_counter()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=32,
                do_sample=False,
                temperature=0.0,
                top_p=1.0,
                repetition_penalty=1.0
            )
        
        if device == "cuda":
            torch.cuda.synchronize()
        end_time = time.perf_counter()
        
        generation_time = end_time - start_time
        generation_times.append(generation_time)
        
        # è§£æç”Ÿæˆçš„ç­”æ¡ˆ
        response_ids = outputs[0][inputs["input_ids"].shape[1]:]
        response = tokenizer.decode(response_ids, skip_special_tokens=True)
        
        # è®¡ç®— F1 åˆ†æ•°
        answers = example["answers"]
        f1 = max([compute_f1(response, answer, tokenizer) for answer in answers])
        f1_scores.append(f1)
        
        print(f"æ ·æœ¬ {i+1}: é¢„æµ‹ç­”æ¡ˆ='{response}', æ ‡å‡†ç­”æ¡ˆ={answers}, F1={f1:.3f}, æ—¶é—´={generation_time:.3f}s")
    
    # è¾“å‡ºç»“æœç»Ÿè®¡
    print("\n=============== æµ‹è¯•ç»“æœ ===============")
    print(f"å¹³å‡ F1 åˆ†æ•°: {np.mean(f1_scores):.4f}")
    print(f"F1 åˆ†æ•°æ ‡å‡†å·®: {np.std(f1_scores):.4f}")
    print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {np.mean(generation_times):.4f}s")
    print(f"ç”Ÿæˆæ—¶é—´æ ‡å‡†å·®: {np.std(generation_times):.4f}s")
    print(f"F1 > 0.5 çš„æ ·æœ¬æ¯”ä¾‹: {np.mean([f1 > 0.5 for f1 in f1_scores]):.2%}")
    
    return {
        'f1_scores': f1_scores,
        'generation_times': generation_times,
        'mean_f1': np.mean(f1_scores),
        'mean_time': np.mean(generation_times)
    }

def main():
    model_path = "/root/model/Llama-2-7b-hf"
    dataset_path = "/workspace/CacheBlend/inputs/musique_s.json"  # éœ€è¦ç¡®è®¤è·¯å¾„
    
    results = test_model_accuracy(model_path, dataset_path)
    
    # ä¿å­˜ç»“æœ
    with open('accuracy_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("ç»“æœå·²ä¿å­˜åˆ° accuracy_results.json")

if __name__ == "__main__":
    main()