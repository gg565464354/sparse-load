#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®æ”¹åçš„å¤´çº§å‘½ä¸­ç‡ç»Ÿè®¡åŠŸèƒ½
"""

import sys
import os
import torch
import transformers
from transformers import AutoTokenizer, AutoConfig

# æ·»åŠ libsè·¯å¾„
sys.path.insert(0, './libs')

from libs.transformers.models.qwen2.modeling_qwen2 import Qwen2ForCausalLM

def test_head_hit_rate():
    """æµ‹è¯•å¤´çº§å‘½ä¸­ç‡ç»Ÿè®¡åŠŸèƒ½"""
    print("=== æµ‹è¯•å¤´çº§å‘½ä¸­ç‡ç»Ÿè®¡åŠŸèƒ½ ===")
    
    # é…ç½®å’Œè®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é…ç½®ç”¨äºæµ‹è¯•
    config = AutoConfig.from_pretrained("Qwen/Qwen2-1.5B")
    config.num_hidden_layers = 2  # åªä½¿ç”¨2å±‚è¿›è¡Œæµ‹è¯•
    config.num_attention_heads = 4  # 4ä¸ªæ³¨æ„åŠ›å¤´
    
    print(f"æµ‹è¯•é…ç½®: {config.num_hidden_layers}å±‚, {config.num_attention_heads}ä¸ªå¤´")
    
    # åˆå§‹åŒ–æ¨¡å‹
    try:
        model = Qwen2ForCausalLM(config)
        model.to(device)
        print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âœ— æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 1
    seq_len = 10
    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len), device=device)
    
    print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    # è¿è¡Œå‰å‘ä¼ æ’­ä»¥ç”Ÿæˆä¸€äº›ç»Ÿè®¡æ•°æ®
    try:
        model.eval()
        with torch.no_grad():
            # å¤šæ¬¡å‰å‘ä¼ æ’­ä»¥ç§¯ç´¯ç»Ÿè®¡æ•°æ®
            for i in range(5):
                outputs = model(input_ids)
                print(f"å®Œæˆç¬¬ {i+1} æ¬¡å‰å‘ä¼ æ’­")
        
        print("âœ“ å‰å‘ä¼ æ’­å®Œæˆ")
    except Exception as e:
        print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥ç»Ÿè®¡æ•°æ®
    try:
        # è·å–ç»Ÿè®¡æ•°æ®
        stats = model.model.get_all_layers_hit_stats()
        print(f"âœ“ è·å–åˆ° {len(stats)} å±‚çš„ç»Ÿè®¡æ•°æ®")
        
        # æ£€æŸ¥ç»Ÿè®¡æ•°æ®ç»“æ„
        for i, stat in enumerate(stats):
            layer_idx = stat.get('layer_idx')
            head_stats = stat.get('head_stats', {})
            average_hit_rate = stat.get('average_hit_rate', 0.0)
            forward_count = stat.get('forward_count', 0)
            
            print(f"Layer {layer_idx}: {len(head_stats)} ä¸ªå¤´, å¹³å‡å‘½ä¸­ç‡: {average_hit_rate:.2%}, Forwardæ¬¡æ•°: {forward_count}")
            
            # æ£€æŸ¥æ¯ä¸ªå¤´çš„ç»Ÿè®¡
            for head_idx, head_stat in head_stats.items():
                hit_tokens = head_stat.get('hit_tokens', 0)
                candidate_tokens = head_stat.get('candidate_tokens', 0)
                hit_rate = head_stat.get('hit_rate', 0.0)
                print(f"  Head {head_idx}: {hit_tokens}/{candidate_tokens} = {hit_rate:.2%}")
        
        print("âœ“ ç»Ÿè®¡æ•°æ®ç»“æ„æ­£ç¡®")
        
    except Exception as e:
        print(f"âœ— ç»Ÿè®¡æ•°æ®æ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•æ‰“å°åŠŸèƒ½
    try:
        print("\n=== æµ‹è¯•è¯¦ç»†ç»Ÿè®¡è¾“å‡º ===")
        model.model.print_hit_rate_summary(detailed=True)
        
        print("\n=== æµ‹è¯•æ±‡æ€»ç»Ÿè®¡è¾“å‡º ===")
        model.model.print_hit_rate_summary(detailed=False)
        
        print("âœ“ æ‰“å°åŠŸèƒ½æ­£å¸¸")
    except Exception as e:
        print(f"âœ— æ‰“å°åŠŸèƒ½å¤±è´¥: {e}")
        return False
    
    print("\n=== æ‰€æœ‰æµ‹è¯•é€šè¿‡! ===")
    return True

if __name__ == "__main__":
    success = test_head_hit_rate()
    if success:
        print("\nğŸ‰ å¤´çº§å‘½ä¸­ç‡ç»Ÿè®¡åŠŸèƒ½å·¥ä½œæ­£å¸¸!")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ä¿®æ”¹ã€‚")
        sys.exit(1) 