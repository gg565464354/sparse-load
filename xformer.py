import torch
import torch.nn.functional as F
import time
import json
from flash_attn.flash_attn_interface import flash_attn_func

# **ğŸš€ 1. æ ‡å‡† SDPAï¼ˆå¸¦ Maskï¼‰**
def sdpa_with_mask(Q, K_padded, V_padded, attn_mask):
    return F.scaled_dot_product_attention(Q, K_padded, V_padded, attn_mask=attn_mask)

# **ğŸš€ 2. ç›´æ¥å»æ‰ Mask çš„ SDPA**
def sdpa_no_mask(Q, K_padded, V_padded):
    return F.scaled_dot_product_attention(Q, K_padded, V_padded)

# **ğŸš€ 3. ä½¿ç”¨ Flash Attention å¤„ç†å˜é•¿ KV Attention**
def flash_attention(Q, K_padded, V_padded, key_lengths):
    B, H, QL, D = Q.shape
    max_seq_len = K_padded.shape[2]

    # è°ƒæ•´ Q å½¢çŠ¶ä¸º Flash Attention éœ€è¦çš„ (B * QL, H, D)
    Q = Q.permute(0, 2, 1, 3).reshape(B * QL, H, D)
    
    # å°† K, V è°ƒæ•´ä¸º Flash Attention éœ€è¦çš„å½¢çŠ¶ (B * H, L, D)
    K_list, V_list = [], []
    for i in range(B * H):
        length = key_lengths[i]
        K_list.append(K_padded[:, i % H, :length, :])  # å–æœ‰æ•ˆçš„ K
        V_list.append(V_padded[:, i % H, :length, :])  # å–æœ‰æ•ˆçš„ V
    
    K_flat = torch.cat(K_list, dim=1)  # (B * H, L, D)
    V_flat = torch.cat(V_list, dim=1)  # (B * H, L, D)
    
    # è®¡ç®— cu_seqlens_k
    cu_seqlens_k = torch.cat([torch.tensor([0], device=Q.device), torch.cumsum(torch.tensor(key_lengths, device=Q.device), dim=0)])

    return flash_attn_func(Q, K_flat, V_flat, dropout_p=0.0, softmax_scale=None, causal=False)

# **ğŸš€ æµ‹è¯•æ€§èƒ½**
def benchmark(func, *args):
    func(*args)  # é¢„çƒ­
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(10):  # è¿è¡Œ 10 æ¬¡å–å¹³å‡å€¼
        func(*args)
    torch.cuda.synchronize()
    end_time = time.time()
    cost = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    print(f"{func.__name__} è¿è¡Œæ—¶é—´: {cost:.2f} ms")
    return cost

# **ğŸš€ é…ç½®å‚æ•°**
BATCH_SIZE = 1
NUM_HEADS = 64
QUERY_LEN = 1
DIM = 128

for i in range(1, 11):
    max_key_len = int(1024 * i)
    min_key_len = int(512 * i)
    
    # **ç”Ÿæˆå˜é•¿ KV é•¿åº¦**
    key_lengths = torch.randint(min_key_len, max_key_len, (BATCH_SIZE * NUM_HEADS,)).tolist()
    
    # **ç”Ÿæˆ Q, K, V**
    Q = torch.randn(BATCH_SIZE, NUM_HEADS, QUERY_LEN, DIM, device="cuda", dtype=torch.float16)
    K_padded = torch.zeros(BATCH_SIZE, NUM_HEADS, max_key_len, DIM, device="cuda", dtype=torch.float16)
    V_padded = torch.zeros(BATCH_SIZE, NUM_HEADS, max_key_len, DIM, device="cuda", dtype=torch.float16)
    attn_mask = torch.ones(BATCH_SIZE, NUM_HEADS, QUERY_LEN, max_key_len, device="cuda", dtype=torch.float16) * float("-inf")
    
    # **å¡«å……ä¸åŒ head çš„ KV å¹¶ç”Ÿæˆ mask**
    for i, length in enumerate(key_lengths):
        K_padded[:, i // NUM_HEADS, :length, :] = torch.randn(BATCH_SIZE, 1, length, DIM, device="cuda", dtype=torch.float16)
        V_padded[:, i // NUM_HEADS, :length, :] = torch.randn(BATCH_SIZE, 1, length, DIM, device="cuda", dtype=torch.float16)
        attn_mask[:, i // NUM_HEADS, :, :length] = 0  # å…è®¸è®¿é—®çš„éƒ¨åˆ†è®¾ä¸º 0
    
    # **ğŸš€ è¿è¡Œæ‰€æœ‰æµ‹è¯•**
    print("\n=== SDPA vs Flash Attention æ€§èƒ½æµ‹è¯• ===")
    hete_attn = benchmark(sdpa_with_mask, Q, K_padded, V_padded, attn_mask)
    common_attn = benchmark(sdpa_no_mask, Q, K_padded, V_padded)
    flash_attn = benchmark(flash_attention, Q, K_padded, V_padded, key_lengths)
    
    # # **ä¿å­˜ç»“æœ**
    # result = {"hete_attn": hete_attn, "common_attn": common_attn, "flash_attn": flash_attn}
    # with open("../cache_test/result/cache_attn_cost.jsonl", "a", encoding="utf-8") as file:
    #     file.write(json.dumps(result, ensure_ascii=False) + "\n")
